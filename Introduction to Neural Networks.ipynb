{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biological Neuron vs Artificial Neuron a.k.a Perceptron\n",
    "\n",
    "\n",
    "<table width=\"800\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/neuron_and_artif_neuron.png\" alt=\"Biological Neuron and Artificial Neuron\" />\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<table width=\"400\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/Formula.png\" />\n",
    "</td>\n",
    "\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<ul>\n",
    "  \n",
    "  <li>In the basic neuron model, the dendrites carry the signal to the cell body where they all get summed <b>(weighted Sum)</b>. If the final sum is above a certain threshold, the neuron can fire, sending a spike along its axon</li>\n",
    "  <li>Each neuron performs a <b>dot product</b> of the input and its weights, adds the bias and applies the <b>non-linearity (i.e activation function)</b>, in this case the sigmoid</li>\n",
    "  <li>The idea is that the synaptic strengths (the weights w) are learnable and <b>control the strength of influence</b> (and its direction: excitory (positive weight) or inhibitory (negative weight)) of one neuron on another. \n",
    "</li>\n",
    "<li>The <b>firing rate</b> of the neuron is modeled by with an activation function f, which represents the frequency of the spikes along the axon</li>\n",
    "  \n",
    "</ul>\n",
    "\n",
    "<i>Source: http://cs231n.github.io/neural-networks-1/</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAND gate using a Perceptron\n",
    "\n",
    "\n",
    "Perceptrons can be used is to compute the elementary logical functions we usually think of as underlying computation, functions such as AND, OR, and NAND. For example, suppose we have a perceptron with two inputs, each with weight −2, and an overall bias of 3. Here's our perceptron:\n",
    "\n",
    "<table width=\"300\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/nand.png\" />\n",
    "</td>\n",
    "</table>\n",
    "\n",
    "Then we see that input 00 produces output 1, since (−2)∗0+(−2)∗0+3=3(−2)∗0+(−2)∗0+3=3 is positive. Here, I've introduced the ∗∗ symbol to make the multiplications explicit. Similar calculations show that the inputs 01 and 10 produce output 1. But the input 11 produces output 0, since (−2)∗1+(−2)∗1+3=−1(−2)∗1+(−2)∗1+3=−1 is negative. \n",
    "And so our perceptron implements a NAND gate!\n",
    "\n",
    "Source: http://neuralnetworksanddeeplearning.com/chap1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorized Form \n",
    "The input data and output lables can be represented in vectorized form. eg, W.T . X + b where W and X are the weights and inputs in a vector form.\n",
    "\n",
    "\n",
    "\n",
    "< markup or pic here >\n",
    "\n",
    "#### Implementation in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#X = [x1,x2] = \n",
    "X = np.array([1,1])\n",
    "#W = [-2,-2]\n",
    "W = np.array([-2,-2])\n",
    "#bias\n",
    "b = 3\n",
    "#output \n",
    "y = ((np.dot(W.T,X) + b) > 0)\n",
    "\n",
    "print y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Logistic Regression and Activation Function\n",
    "\n",
    "<table width=\"600\" border=\"1\" cellpadding=\"5\">\n",
    "<tr>\n",
    "<td align=\"center\" valign=\"center\">\n",
    "<img src=\"images/sigmoid_vs_linear.png\" alt=\"Biological Neuron and Artificial Neuron\" />\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<ul>\n",
    "<li>The logistic regression formula is <b>derived from the standard linear equation for a straight line</b> that is y=mx + b\n",
    "</li>\n",
    "<li>Using the <b>Sigmoid function (a.k.a Activation Function)</b>, the standard linear formula is transformed to the logistic regression formula\n",
    "</li>\n",
    "<li>This logistic regression function is useful for <b>predicting the class</b> i.e the output has disceret / <b>catagorical values</b>. (versus linear regression where output is a continious variable)\n",
    "</li>\n",
    "<li>The Activation Function therefore <b>adds a non-linearity</b> to the linear function. \n",
    "</li>\n",
    "</ul>\n",
    " \n",
    "\n",
    "\n",
    "#### Relu Activation Function\n",
    "(TODO)\n",
    "#### tanh Activation Function\n",
    "(TODO)\n",
    "#### Softmax Activation Function\n",
    "(TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Linear Algebra \n",
    "\n",
    "Source : http://rlhick.people.wm.edu/stories/linear-algebra-python-basics.html\n",
    "\n",
    "From the example above, [x1,x2,x3].[w1,w2,w3]  = [y]   \n",
    "(nx1) . (1xn) = (1x1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement XOR function \n",
    "\n",
    "<img src=\"images/XOR_gate.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "<img src=\"images/XOR_table.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "Source: http://toritris.weebly.com/perceptron-5-xor-how--why-neurons-work-together.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
